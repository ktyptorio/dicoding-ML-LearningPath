{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project - Rock_Paper_Scissors",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ktyptorio/dicoding-ML-LearningPath/blob/main/Project_Rock_Paper_Scissors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AATzcBWpGuyG"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRSrhuatHGi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc9c4e3-e2f2-4771-9d84-df257ea98651"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "  https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip \\\n",
        "  -O /tmp/rockpaperscissors.zip"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-12 20:20:21--  https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/391417272/7eb836f2-695b-4a46-9c78-b65867166957?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220712%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220712T202021Z&X-Amz-Expires=300&X-Amz-Signature=0799659b9c45e38771e8c55fc5791f2300073eb8b4037987278828bf2bbf48ba&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=391417272&response-content-disposition=attachment%3B%20filename%3Drockpaperscissors.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-07-12 20:20:21--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/391417272/7eb836f2-695b-4a46-9c78-b65867166957?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220712%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220712T202021Z&X-Amz-Expires=300&X-Amz-Signature=0799659b9c45e38771e8c55fc5791f2300073eb8b4037987278828bf2bbf48ba&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=391417272&response-content-disposition=attachment%3B%20filename%3Drockpaperscissors.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 322873683 (308M) [application/octet-stream]\n",
            "Saving to: ‘/tmp/rockpaperscissors.zip’\n",
            "\n",
            "/tmp/rockpapersciss 100%[===================>] 307.92M  65.0MB/s    in 4.7s    \n",
            "\n",
            "2022-07-12 20:20:26 (66.0 MB/s) - ‘/tmp/rockpaperscissors.zip’ saved [322873683/322873683]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTBu2oJfHIbH"
      },
      "source": [
        "# melakukan ekstraksi pada file zip\n",
        "import zipfile,os\n",
        "local_zip = '/tmp/rockpaperscissors.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/rockpaperscissors/rps-cv-images'"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install split-folders\n",
        "import splitfolders\n",
        "\n",
        "splitfolders.ratio(base_dir, output=\"/tmp/data-models\", seed=1, ratio=(.6, .4), group_prefix=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdCVyMKEvp96",
        "outputId": "54db8746-6f27-4fb3-b48a-2b47dbff1a53"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 2188 files [00:00, 3497.89 files/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-lp90PCHQRZ"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=20,\n",
        "                    horizontal_flip=True,\n",
        "                    shear_range = 0.2,\n",
        "                    fill_mode = 'nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "                    rescale=1./255)"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jZFBH10HSI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbfba54-98b0-4b84-93a3-eaaef40fe2c4"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "        \"/tmp/data-models/train\",\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        \"/tmp/data-models/val\",\n",
        "        target_size=(150, 150),\n",
        "        batch_size=32, \n",
        "        class_mode='categorical')"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1312 images belonging to 3 classes.\n",
            "Found 876 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ_KwTQJHT-U"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgB1dDJ1KypF"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbvLD0cVHXJ6"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVmUsHpzHX3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ead358-ef42-4010-872f-7ce7b0c1a442"
      },
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=25,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=10,\n",
        "      verbose=1)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "25/25 [==============================] - 7s 266ms/step - loss: 1.0945 - accuracy: 0.3837 - val_loss: 1.0636 - val_accuracy: 0.5813\n",
            "Epoch 2/30\n",
            "25/25 [==============================] - 6s 254ms/step - loss: 1.0785 - accuracy: 0.4000 - val_loss: 1.0872 - val_accuracy: 0.3438\n",
            "Epoch 3/30\n",
            "25/25 [==============================] - 7s 290ms/step - loss: 1.0941 - accuracy: 0.4325 - val_loss: 1.0943 - val_accuracy: 0.5250\n",
            "Epoch 4/30\n",
            "25/25 [==============================] - 6s 255ms/step - loss: 1.0898 - accuracy: 0.4300 - val_loss: 1.0702 - val_accuracy: 0.3313\n",
            "Epoch 5/30\n",
            "25/25 [==============================] - 6s 254ms/step - loss: 1.0526 - accuracy: 0.4762 - val_loss: 1.0986 - val_accuracy: 0.3406\n",
            "Epoch 6/30\n",
            "25/25 [==============================] - 6s 253ms/step - loss: 1.0770 - accuracy: 0.4187 - val_loss: 0.9689 - val_accuracy: 0.7437\n",
            "Epoch 7/30\n",
            "25/25 [==============================] - 7s 267ms/step - loss: 1.1044 - accuracy: 0.4787 - val_loss: 1.1089 - val_accuracy: 0.3406\n",
            "Epoch 8/30\n",
            "25/25 [==============================] - 6s 252ms/step - loss: 1.0621 - accuracy: 0.4588 - val_loss: 0.9485 - val_accuracy: 0.6469\n",
            "Epoch 9/30\n",
            "25/25 [==============================] - 6s 253ms/step - loss: 0.9679 - accuracy: 0.5750 - val_loss: 1.1356 - val_accuracy: 0.3187\n",
            "Epoch 10/30\n",
            "25/25 [==============================] - 6s 253ms/step - loss: 0.8297 - accuracy: 0.6388 - val_loss: 0.6582 - val_accuracy: 0.7750\n",
            "Epoch 11/30\n",
            "25/25 [==============================] - 7s 284ms/step - loss: 0.7142 - accuracy: 0.7412 - val_loss: 0.4264 - val_accuracy: 0.8687\n",
            "Epoch 12/30\n",
            "25/25 [==============================] - 7s 283ms/step - loss: 0.7236 - accuracy: 0.7237 - val_loss: 0.4867 - val_accuracy: 0.8438\n",
            "Epoch 13/30\n",
            "25/25 [==============================] - 6s 255ms/step - loss: 0.3949 - accuracy: 0.8413 - val_loss: 0.3159 - val_accuracy: 0.8781\n",
            "Epoch 14/30\n",
            "25/25 [==============================] - 6s 259ms/step - loss: 0.3362 - accuracy: 0.8800 - val_loss: 0.2613 - val_accuracy: 0.9125\n",
            "Epoch 15/30\n",
            "25/25 [==============================] - 7s 291ms/step - loss: 0.2511 - accuracy: 0.9137 - val_loss: 0.2820 - val_accuracy: 0.9000\n",
            "Epoch 16/30\n",
            "25/25 [==============================] - 6s 256ms/step - loss: 0.2433 - accuracy: 0.9175 - val_loss: 0.2105 - val_accuracy: 0.9156\n",
            "Epoch 17/30\n",
            "25/25 [==============================] - 6s 256ms/step - loss: 0.2149 - accuracy: 0.9337 - val_loss: 0.2898 - val_accuracy: 0.8719\n",
            "Epoch 18/30\n",
            "25/25 [==============================] - 6s 256ms/step - loss: 0.1566 - accuracy: 0.9513 - val_loss: 0.1202 - val_accuracy: 0.9531\n",
            "Epoch 19/30\n",
            "25/25 [==============================] - 7s 271ms/step - loss: 0.1626 - accuracy: 0.9388 - val_loss: 0.1885 - val_accuracy: 0.9406\n",
            "Epoch 20/30\n",
            "25/25 [==============================] - 6s 255ms/step - loss: 0.1376 - accuracy: 0.9513 - val_loss: 0.0928 - val_accuracy: 0.9656\n",
            "Epoch 21/30\n",
            "25/25 [==============================] - 6s 251ms/step - loss: 0.1436 - accuracy: 0.9513 - val_loss: 0.0872 - val_accuracy: 0.9750\n",
            "Epoch 22/30\n",
            "25/25 [==============================] - 6s 257ms/step - loss: 0.1343 - accuracy: 0.9525 - val_loss: 0.0810 - val_accuracy: 0.9781\n",
            "Epoch 23/30\n",
            "25/25 [==============================] - 7s 268ms/step - loss: 0.1068 - accuracy: 0.9712 - val_loss: 0.1213 - val_accuracy: 0.9688\n",
            "Epoch 24/30\n",
            "25/25 [==============================] - 6s 253ms/step - loss: 0.1090 - accuracy: 0.9588 - val_loss: 0.2458 - val_accuracy: 0.9250\n",
            "Epoch 25/30\n",
            "25/25 [==============================] - 6s 255ms/step - loss: 0.0871 - accuracy: 0.9688 - val_loss: 0.0964 - val_accuracy: 0.9750\n",
            "Epoch 26/30\n",
            "25/25 [==============================] - 7s 273ms/step - loss: 0.0919 - accuracy: 0.9638 - val_loss: 0.1257 - val_accuracy: 0.9625\n",
            "Epoch 27/30\n",
            "25/25 [==============================] - 7s 273ms/step - loss: 0.1085 - accuracy: 0.9700 - val_loss: 0.0473 - val_accuracy: 0.9844\n",
            "Epoch 28/30\n",
            "25/25 [==============================] - 7s 277ms/step - loss: 0.0841 - accuracy: 0.9787 - val_loss: 0.1023 - val_accuracy: 0.9688\n",
            "Epoch 29/30\n",
            "25/25 [==============================] - 6s 261ms/step - loss: 0.0572 - accuracy: 0.9800 - val_loss: 0.1420 - val_accuracy: 0.9563\n",
            "Epoch 30/30\n",
            "25/25 [==============================] - 7s 276ms/step - loss: 0.0795 - accuracy: 0.9700 - val_loss: 0.0785 - val_accuracy: 0.9812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(train_generator,steps=25,verbose=0)\n",
        "print('Accuracy on training data: {:.4f} \\nLoss on training data: {:.4f}'.format(acc,loss),'\\n')\n",
        " \n",
        "loss, acc = model.evaluate(validation_generator,steps=10,verbose=0)\n",
        "print('Accuracy on test data: {:.4f} \\nLoss on test data: {:.4f}'.format(acc,loss),'\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnY4fHOl3-p4",
        "outputId": "b6e61880-ee9f-4ebb-91f5-9947bbf5ceda"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on training data: 0.9775 \n",
            "Loss on training data: 0.0720 \n",
            "\n",
            "Accuracy on test data: 0.9812 \n",
            "Loss on test data: 0.0492 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQOoO8GTHbOR"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(150,150)) \n",
        "\n",
        "  imgplot = plt.imshow(img)\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "\n",
        "  classes = np.argmax(model.predict(images, batch_size=32))\n",
        "  print(fn)\n",
        "  print(\"%f\" % (classes))\n",
        "  if classes==0:\n",
        "   print('paper')\n",
        "  elif classes==1:\n",
        "   print('rock')\n",
        "  else:\n",
        "    print('scissors')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}